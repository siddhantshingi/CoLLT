# CoLLT
Contrastive Learning for Long Document Transformers 
To do:
1. Final Report
  * Introduction, Related work, Datasets - Copy from proposal
  * Baselines - Nidhi
  * Datasets - Alok ( Add a histogram of text length distribution)
  * Approach - Siddhant and Mehek
  * Add barlow twins explanation
  * Experiment 1 results
  * Experiment 2 results
  * Experiment 3 results
  * Error analysis
  * Conclusion and Future work
2. Experiment 1
  * Baseline 1 - Run BERT on IMDB 1000 subset, just fine tune
  * Baseline 2 - Run BERT on IMDB 1000 subset, model tune and fine tune
  * Baseline 3 - Run longformer on IMDB 1000 subset, model tune and fine tune
  * Contrastive on BERT, model tune and fine tune
  * Contrastive on BERT, just fine tune
3. Experiment 2
  * Contrastive on longformer, model tune and fine tune
4. Experiment 3
  * Trying different augmentation types. (Random chunk, non overlapping chunk , overlapping chunk)
